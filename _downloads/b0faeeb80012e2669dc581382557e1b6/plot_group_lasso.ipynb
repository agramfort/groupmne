{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nMulti-subject joint source localization with multi-task models\n==============================================================\n\nThe aim of this tutorial is to show how to leverage functional similarity\nacross subjects to improve source localization. For that purpose we use the\nthe high frequency SEF MEG dataset of (Nurminen et al., 2017) which provides\nMEG and MRI data for two subjects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Hicham Janati (hicham.janati@inria.fr)\n#\n# License: BSD (3-clause)\n\nimport os\nimport os.path as op\nimport mne\nfrom mne.datasets import hf_sef\nfrom matplotlib import pyplot as plt\n\nfrom groupmne import group_model\nfrom groupmne.inverse import compute_group_inverse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download and process MEG data\n-----------------------------\n\nAveraged MEG data (and MRI) are provided in \"evoked\".\nWe assume that the noise covariance matrices have been computed\nusing the raw data as shown in example plot_process_meg.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_path = hf_sef.data_path(\"evoked\")\nmeg_path = data_path + \"/MEG/\"\n\ndata_path = op.expanduser(data_path)\nsubjects_dir = data_path + \"/subjects/\"\nos.environ['SUBJECTS_DIR'] = subjects_dir\n\nev_name_s = [meg_path + s for s in [\"subject_a/sef_right-ave.fif\",\n             \"subject_b/hf_sef_15min-ave.fif\"]]\ncov_name_s = [meg_path + s for s in [\"subject_a/sef-cov.fif\",\n              \"subject_b/sef-cov.fif\"]]\n\nevoked_s = []\nf, axes = plt.subplots(1, 2, sharey=True)\nfor ax, ev_name, ll in zip(axes.ravel(), ev_name_s, [\"a\", \"b\"]):\n    ev = mne.read_evokeds(ev_name)[0]\n    evoked_s.append(ev)\n    picks = mne.pick_types(ev.info, meg=\"grad\")\n    ev.plot(picks=picks, axes=ax, show=False)\n    ax.set_title(\"Subject %s\" % ll, fontsize=15)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Source and forward modeling\n---------------------------\nTo guarantee an alignment across subjects, we start by\ncomputing (or reading if available) the source space of the average\nsubject of freesurfer `fsaverage`\nIf fsaverage is not available, it will be fetched to the data_path\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# resolution = 3\n# spacing = \"ico%d\" % resolution\n# src_ref = group_model.get_src_reference(spacing=spacing,\n#                                         subjects_dir=subjects_dir)\n#\n# ###################################################################\n#\n# # the function group_model.compute_fwd morphs the source space src_ref to\n# # the surface of each subject by mapping the sulci and gyri patterns\n# # and computes their forward operators\n#\n# subjects = [\"subject_a\", \"subject_b\"]\n# trans_fname_s = [meg_path + \"%s/sef-trans.fif\" % s for s in subjects]\n# bem_fname_s = [subjects_dir + \"%s/bem/%s-5120-bem-sol.fif\" % (s, s)\n#                for s in subjects]\n# # n_jobs = 2\n# # parallel, run_func, _ = parallel_func(group_model.compute_fwd,\n# #                                       n_jobs=n_jobs)\n# #\n# # fwds = parallel(run_func(s, src_ref, info, trans, bem,  mindist=3)\n# #                 for s, info, trans, bem in zip(subjects, raw_name_s,\n# #                                                trans_fname_s, bem_fname_s))\n#\n# fwds = []\n# noise_cov_s = []\n# for s, info, trans, bem, cov_name in zip(subjects, ev_name_s,\n#                                          trans_fname_s, bem_fname_s,\n#                                          cov_name_s):\n#     fwd = group_model.compute_fwd(s, src_ref, info, trans, bem,  mindist=3)\n#     fwds.append(fwd)\n#     cov = mne.read_cov(cov_name)\n#     noise_cov_s.append(cov)\n#     del fwd, cov\n#\n# ############################################\n# # We can now compute the data of the inverse problem.\n# # `group_info` is a dictionary that contains the selected channels and the\n# # alignment maps between src_ref and the subjects which are required if you\n# # want to plot source estimates on the brain surface of each subject.\n#\n# gains, M, group_info = \\\n#     group_model.compute_inv_data(fwds, src_ref, evoked_s, noise_cov_s,\n#                                  ch_type=\"grad\", tmin=0.02, tmax=0.04)\n# print(\"(# subjects, # channels, # sources) = \", gains.shape)\n# print(\"(# subjects, # channels, # time points) = \", M.shape)\n#\n# ############################################\n# # Solve the inverse problems\n# # --------------------------\n# #\n# stcs, log = compute_group_inverse(gains, M, group_info,\n#                                   method=\"grouplasso\",\n#                                   depth=0.9, alpha=0.1, return_stc=True,\n#                                   n_jobs=4)\n# t = 0.025\n# t_idx = stcs[0].time_as_index(t)\n# for stc, subject in zip(stcs, subjects):\n#     m = abs(stc.data[:group_info[\"n_sources\"][0], t_idx]).max()\n#     surfer_kwargs = dict(\n#         clim=dict(kind='value', pos_lims=[0., 0.1 * m, m]),\n#         hemi='lh', subjects_dir=subjects_dir, views='lateral',\n#         initial_time=t, time_unit='s', size=(800, 800),\n#         smoothing_steps=5)\n#     brain = stc.plot(**surfer_kwargs)\n#     brain.add_text(0.1, 0.9, subject, \"title\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}